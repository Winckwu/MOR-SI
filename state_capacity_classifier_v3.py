#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¿åºœé‡‡è´­åˆåŒå›½å®¶èƒ½åŠ›åˆ†ç±»å™¨ V3 - é«˜å‡†ç¡®åº¦ç‰ˆæœ¬
State Capacity Contract Classifier V3 - High Accuracy Version

ç›®æ ‡ï¼šå‡†ç¡®ç‡ > 90%

ä¼˜åŒ–ç­–ç•¥ï¼š
1. ä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼ˆ2012-2015ï¼Œçº¦2ä¸‡æ¡ï¼‰
2. æ›´ç²¾ç»†çš„å…³é”®è¯æƒé‡ç³»ç»Ÿ
3. å¤šç‰¹å¾èåˆï¼ˆæ–‡æœ¬+è¡Œä¸š+é‡‡è´­äººç±»å‹ï¼‰
4. é›†æˆå­¦ä¹ ï¼ˆæŠ•ç¥¨åˆ†ç±»å™¨ï¼‰
5. è¶…å‚æ•°ä¼˜åŒ–

åŸºäº Berwick & Christia (2018) ã€ŠState Capacity Reduxã€‹è®ºæ–‡æ¡†æ¶
"""

import pandas as pd
import numpy as np
import re
import warnings
from collections import Counter
warnings.filterwarnings('ignore')

# æœºå™¨å­¦ä¹ åº“
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.utils import resample
from scipy.sparse import hstack, csr_matrix

# å¯è§†åŒ–
import matplotlib.pyplot as plt
import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS']
matplotlib.rcParams['axes.unicode_minus'] = False

RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)


# ============================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šé«˜ç²¾åº¦æ ‡æ³¨å™¨
# ============================================================================

class HighAccuracyLabeler:
    """
    é«˜ç²¾åº¦æ ‡æ³¨å™¨ - ä½¿ç”¨åˆ†å±‚è§„åˆ™å’Œç½®ä¿¡åº¦è¿‡æ»¤
    """

    def __init__(self):
        # æ±²å–èƒ½åŠ› - å¼ºè°ƒ"èµ„æºè·å–"
        self.extractive_rules = {
            # å†³å®šæ€§å…³é”®è¯ï¼ˆç›´æ¥åˆ¤å®šï¼‰
            'decisive': [
                'ç¨åŠ¡å±€', 'åœ°ç¨å±€', 'å›½ç¨å±€', 'ç¨åŠ¡',
                'è´¢æ”¿å±€', 'è´¢æ”¿å…', 'è´¢æ”¿',
                'å®¡è®¡å±€', 'å®¡è®¡å…', 'å®¡è®¡',
                'å›½åœŸå±€', 'å›½åœŸèµ„æº', 'åœŸåœ°å‚¨å¤‡', 'åœŸåœ°å‡ºè®©',
                'çŸ¿äº§èµ„æº', 'çŸ¿æƒ', 'é‡‡çŸ¿æƒ',
                'èµ„äº§è¯„ä¼°', 'èµ„äº§æ¸…æŸ¥', 'å›½æœ‰èµ„äº§',
                'å¾åœ°æ‹†è¿', 'åœŸåœ°å¾æ”¶', 'æˆ¿å±‹å¾æ”¶',
            ],
            # å¼ºä¿¡å·ï¼ˆæƒé‡5ï¼‰
            'strong': [
                'é¢„ç®—', 'å†³ç®—', 'è´¢åŠ¡ç®¡ç†', 'ä¼šè®¡', 'å‡ºçº³',
                'äº§æƒç™»è®°', 'ä¸åŠ¨äº§ç™»è®°', 'æˆ¿äº§ç™»è®°',
                'åœ°ç±', 'æµ‹ç»˜', 'ç¡®æƒ',
                'èµ„äº§', 'èµ„æº', 'çŸ¿äº§',
            ],
            # ä¸­ç­‰ä¿¡å·ï¼ˆæƒé‡2ï¼‰
            'medium': [
                'è¯„ä¼°', 'é‰´å®š', 'ä»·å€¼',
                'é“¶è¡Œ', 'é‡‘è', 'è´·æ¬¾', 'èèµ„',
            ]
        }

        # åè°ƒèƒ½åŠ› - å¼ºè°ƒ"é›†ä½“è¡ŒåŠ¨ç»„ç»‡"
        self.coordination_rules = {
            'decisive': [
                'å¸‚æ”¿å·¥ç¨‹', 'é“è·¯å·¥ç¨‹', 'å…¬è·¯å·¥ç¨‹', 'æ¡¥æ¢å·¥ç¨‹',
                'æ°´åˆ©å·¥ç¨‹', 'ç”µåŠ›å·¥ç¨‹', 'é€šä¿¡å·¥ç¨‹',
                'ä½å»ºå±€', 'äº¤é€šå±€', 'å…¬è·¯å±€', 'å¸‚æ”¿',
                'ä¿¡æ¯åŒ–å»ºè®¾', 'ç”µå­æ”¿åŠ¡', 'æ™ºæ…§åŸå¸‚',
            ],
            'strong': [
                'é“è·¯', 'å…¬è·¯', 'æ¡¥æ¢', 'éš§é“', 'äº¤é€š',
                'æ°´åˆ©', 'ç”µåŠ›', 'ä¾›ç”µ', 'ä¾›æ°´', 'æ’æ°´', 'ç®¡ç½‘', 'ç‡ƒæ°”',
                'é€šè®¯', 'é€šä¿¡', 'ç½‘ç»œ', 'ä¿¡æ¯åŒ–',
                'å»ºè®¾', 'æ–½å·¥', 'æ”¹é€ ', 'ä¿®ç¼®', 'ç»´ä¿®',
                'è£…ä¿®', 'è£…é¥°', 'ç»¿åŒ–', 'ç¯å«',
                'æ”¿åºœé‡‡è´­', 'åŠå…¬è®¾å¤‡', 'åŠå…¬å®¶å…·', 'å…¬åŠ¡è½¦',
            ],
            'medium': [
                'åŠå…¬', 'ä¼šè®®', 'æ¡£æ¡ˆ', 'å°åˆ·', 'è½¦è¾†',
                'è§„åˆ’', 'è®¾è®¡', 'å’¨è¯¢', 'ç›‘ç†',
                'è®¾å¤‡', 'å®¶å…·', 'ç©ºè°ƒ', 'ç”µè„‘', 'è®¡ç®—æœº', 'æ‰“å°æœº',
            ]
        }

        # åˆè§„èƒ½åŠ› - å¼ºè°ƒ"å…¬å…±æœåŠ¡æä¾›"
        self.compliance_rules = {
            'decisive': [
                # æ•™è‚²ç±»
                'æ•™è‚²å±€', 'æ•™å§”', 'å­¦æ ¡', 'å¤§å­¦', 'å­¦é™¢', 'ä¸­å­¦', 'å°å­¦', 'å¹¼å„¿å›­',
                'æ•™å­¦è®¾å¤‡', 'æ•™å­¦ä»ªå™¨', 'å®éªŒå®¤è®¾å¤‡', 'å®è®­è®¾å¤‡',
                'å›¾ä¹¦é¦†', 'å›¾ä¹¦é‡‡è´­', 'æ•™æ',
                # åŒ»ç–—ç±»
                'åŒ»é™¢', 'å«ç”Ÿé™¢', 'å«ç”Ÿå±€', 'å«å¥å§”', 'ç–¾æ§ä¸­å¿ƒ', 'ç–¾æ§',
                'åŒ»ç–—è®¾å¤‡', 'åŒ»ç–—å™¨æ¢°', 'è¯Šæ–­è®¾å¤‡',
                'CT', 'MRI', 'DR', 'Bè¶…', 'å½©è¶…', 'æ‰‹æœ¯', 'Xå…‰',
                'è¯å“', 'ç–«è‹—', 'åŒ»è¯',
                # å…¬å…±å®‰å…¨
                'å…¬å®‰å±€', 'æ´¾å‡ºæ‰€', 'è­¦åŠ¡', 'æ‰§æ³•', 'å¸æ³•å±€',
                'ç›‘æ§ç³»ç»Ÿ', 'å®‰é˜²ç³»ç»Ÿ', 'æ¶ˆé˜²',
            ],
            'strong': [
                'æ•™è‚²', 'æ•™å­¦', 'åŸ¹è®­', 'å­¦ä¹ ', 'è¯¾ç¨‹', 'å®éªŒ', 'å®è®­',
                'åŒ»ç–—', 'å«ç”Ÿ', 'å¥åº·', 'è¯Šç–—', 'æŠ¤ç†', 'åº·å¤', 'æ£€éªŒ', 'æ£€æµ‹',
                'å®‰é˜²', 'ç›‘æ§', 'å®‰ä¿', 'æ¶ˆé˜²', 'åº”æ€¥',
                'ç¯ä¿', 'ç¯å¢ƒ', 'æ±¡æŸ“æ²»ç†', 'åƒåœ¾å¤„ç†',
                'å…»è€', 'ç¦åˆ©', 'æ•‘åŠ©', 'ç¤¾åŒº',
            ],
            'medium': [
                'ä½“è‚²', 'è¿åŠ¨', 'å¥èº«', 'æ–‡åŒ–', 'è‰ºæœ¯',
                'å›¾ä¹¦', 'é˜…è¯»', 'å¤šåª’ä½“', 'å½•æ’­',
            ]
        }

        # è¡Œä¸šå†³å®šæ€§æ˜ å°„
        self.industry_decisive = {
            # åˆè§„èƒ½åŠ›
            'æ™®é€šé«˜ç­‰æ•™è‚²': 'compliance',
            'ä¸­ç­‰èŒä¸šå­¦æ ¡æ•™è‚²': 'compliance',
            'æ™®é€šå°å­¦æ•™è‚²': 'compliance',
            'æ™®é€šåˆä¸­æ•™è‚²': 'compliance',
            'å­¦å‰æ•™è‚²': 'compliance',
            'ç‰¹æ®Šæ•™è‚²': 'compliance',
            'ç»¼åˆåŒ»é™¢': 'compliance',
            'ä¸“ç§‘åŒ»é™¢': 'compliance',
            'ç–¾ç—…é¢„é˜²æ§åˆ¶ä¸­å¿ƒ': 'compliance',
            'å«ç”Ÿå’Œç¤¾ä¼šå·¥ä½œ': 'compliance',
            'ç¤¾ä¼šç¦åˆ©': 'compliance',

            # æ±²å–èƒ½åŠ›
            'è´¢æ”¿': 'extractive',
            'ç¨åŠ¡': 'extractive',
            'é‡‘èä¸š': 'extractive',

            # åè°ƒèƒ½åŠ›
            'åœŸæœ¨å·¥ç¨‹å»ºç­‘ä¸š': 'coordination',
            'å»ºç­‘è£…é¥°ä¸š': 'coordination',
            'é“è·¯è¿è¾“ä¸š': 'coordination',
        }

    def _match_rules(self, text, rules):
        """åŒ¹é…è§„åˆ™å¹¶è¿”å›å¾—åˆ†"""
        if pd.isna(text):
            return 0, False, []

        text = str(text).lower()
        score = 0
        is_decisive = False
        matched = []

        # æ£€æŸ¥å†³å®šæ€§å…³é”®è¯
        for kw in rules.get('decisive', []):
            if kw.lower() in text:
                is_decisive = True
                score += 10
                matched.append((kw, 'decisive'))

        # å¼ºä¿¡å·
        for kw in rules.get('strong', []):
            if kw.lower() in text:
                score += 5
                matched.append((kw, 'strong'))

        # ä¸­ç­‰ä¿¡å·
        for kw in rules.get('medium', []):
            if kw.lower() in text:
                score += 2
                matched.append((kw, 'medium'))

        return score, is_decisive, matched

    def label_single(self, row):
        """æ ‡æ³¨å•æ¡è®°å½•"""
        # è·å–æ–‡æœ¬
        contract_name = str(row.get('åˆåŒåç§°', '')) if not pd.isna(row.get('åˆåŒåç§°')) else ''
        subject_name = str(row.get('ä¸»è¦æ ‡çš„åç§°', '')) if not pd.isna(row.get('ä¸»è¦æ ‡çš„åç§°')) else ''
        industry = str(row.get('æ‰€å±è¡Œä¸š', '')) if not pd.isna(row.get('æ‰€å±è¡Œä¸š')) else ''
        purchaser = str(row.get('é‡‡è´­äºº', '')) if not pd.isna(row.get('é‡‡è´­äºº')) else ''

        full_text = f"{contract_name} {subject_name} {purchaser}"

        # è¡Œä¸šå†³å®šæ€§åˆ¤æ–­
        for ind_key, label in self.industry_decisive.items():
            if ind_key in industry:
                return label, 0.95, f'è¡Œä¸šå†³å®š:{ind_key}'

        # è§„åˆ™åŒ¹é…
        ext_score, ext_decisive, ext_matched = self._match_rules(full_text, self.extractive_rules)
        coord_score, coord_decisive, coord_matched = self._match_rules(full_text, self.coordination_rules)
        comp_score, comp_decisive, comp_matched = self._match_rules(full_text, self.compliance_rules)

        # é‡‡è´­äººç±»å‹åˆ¤æ–­
        purchaser_lower = purchaser.lower()
        if any(kw in purchaser_lower for kw in ['å­¦æ ¡', 'å¤§å­¦', 'å­¦é™¢', 'å°å­¦', 'ä¸­å­¦', 'å¹¼å„¿å›­', 'æ•™è‚²']):
            comp_score += 8
        elif any(kw in purchaser_lower for kw in ['åŒ»é™¢', 'å«ç”Ÿé™¢', 'ç–¾æ§', 'å«ç”Ÿ']):
            comp_score += 8
        elif any(kw in purchaser_lower for kw in ['ç¨åŠ¡', 'è´¢æ”¿', 'å®¡è®¡', 'å›½åœŸ']):
            ext_score += 8
        elif any(kw in purchaser_lower for kw in ['ä½å»º', 'äº¤é€š', 'å…¬è·¯', 'å¸‚æ”¿', 'æ°´åˆ©']):
            coord_score += 5

        # å†³å®šæ€§å…³é”®è¯ä¼˜å…ˆ
        if ext_decisive and not coord_decisive and not comp_decisive:
            return 'extractive', 0.9, f'å†³å®šæ€§åŒ¹é…'
        if coord_decisive and not ext_decisive and not comp_decisive:
            return 'coordination', 0.9, f'å†³å®šæ€§åŒ¹é…'
        if comp_decisive and not ext_decisive and not coord_decisive:
            return 'compliance', 0.9, f'å†³å®šæ€§åŒ¹é…'

        # å¾—åˆ†åˆ¤æ–­
        scores = {
            'extractive': ext_score,
            'coordination': coord_score,
            'compliance': comp_score
        }

        max_score = max(scores.values())
        total_score = sum(scores.values())

        if max_score == 0:
            return 'coordination', 0.1, 'é»˜è®¤'

        label = max(scores, key=scores.get)

        # è®¡ç®—ç½®ä¿¡åº¦
        if total_score > 0:
            margin = max_score - sorted(scores.values(), reverse=True)[1]
            confidence = min(0.5 + (margin / total_score) * 0.5, 0.95)
        else:
            confidence = 0.1

        reason = f'E:{ext_score},C:{coord_score},P:{comp_score}'
        return label, confidence, reason

    def label_dataframe(self, df):
        """æ ‡æ³¨æ•´ä¸ªæ•°æ®æ¡†"""
        results = df.apply(self.label_single, axis=1)

        df_labeled = df.copy()
        df_labeled['capacity_label'] = [r[0] for r in results]
        df_labeled['label_confidence'] = [r[1] for r in results]
        df_labeled['label_reason'] = [r[2] for r in results]

        return df_labeled


# ============================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šæ•°æ®åŠ è½½å’Œå¤„ç†
# ============================================================================

def load_all_data():
    """åŠ è½½æ‰€æœ‰å¹´ä»½æ•°æ®"""
    print("=" * 60)
    print("ğŸ“‚ åŠ è½½æ•°æ®...")
    print("=" * 60)

    dfs = []

    # åŠ è½½.dtaæ–‡ä»¶
    for year in ['2012', '2013', '2014']:
        try:
            df = pd.read_stata(f'{year}.dta')
            print(f"  {year}å¹´ (.dta): {len(df)} æ¡")
            dfs.append(df)
        except Exception as e:
            print(f"  {year}å¹´: åŠ è½½å¤±è´¥ - {e}")

    # åŠ è½½2015å¹´Excelæ–‡ä»¶
    try:
        df_2015 = pd.read_excel('2015.xls')
        print(f"  2015å¹´ (.xls): {len(df_2015)} æ¡")
        dfs.append(df_2015)
    except Exception as e:
        print(f"  2015å¹´: åŠ è½½å¤±è´¥ - {e}")

    df_all = pd.concat(dfs, ignore_index=True)
    print(f"\n  ğŸ“Š æ€»è®¡: {len(df_all)} æ¡è®°å½•")

    return df_all


def preprocess_text(text):
    """æ–‡æœ¬é¢„å¤„ç†"""
    if pd.isna(text):
        return ''
    text = str(text)
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def create_features(row):
    """åˆ›å»ºç»„åˆç‰¹å¾"""
    texts = []
    for col in ['åˆåŒåç§°', 'ä¸»è¦æ ‡çš„åç§°', 'æ‰€å±è¡Œä¸š', 'é‡‡è´­æ–¹å¼', 'é‡‡è´­äºº']:
        val = row.get(col)
        if val is not None and not pd.isna(val):
            texts.append(str(val))
    return ' '.join(texts)


def balance_classes(df, target_col='capacity_label'):
    """ç±»åˆ«å¹³è¡¡ - ä½¿ç”¨SMOTEé£æ ¼çš„ä¸Šé‡‡æ ·"""
    print("\n  âš–ï¸ ç±»åˆ«å¹³è¡¡å¤„ç†...")

    class_counts = df[target_col].value_counts()
    print(f"  åŸå§‹åˆ†å¸ƒ: {dict(class_counts)}")

    max_size = class_counts.max()
    dfs = []

    for label in class_counts.index:
        df_subset = df[df[target_col] == label]
        if len(df_subset) < max_size:
            df_upsampled = resample(df_subset,
                                    replace=True,
                                    n_samples=max_size,
                                    random_state=RANDOM_STATE)
            dfs.append(df_upsampled)
        else:
            dfs.append(df_subset)

    df_balanced = pd.concat(dfs)
    new_counts = df_balanced[target_col].value_counts()
    print(f"  å¹³è¡¡å: {dict(new_counts)}")

    return df_balanced.reset_index(drop=True)


# ============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šé«˜å‡†ç¡®åº¦åˆ†ç±»å™¨
# ============================================================================

class HighAccuracyClassifier:
    """é«˜å‡†ç¡®åº¦åˆ†ç±»å™¨ - ä½¿ç”¨é›†æˆå­¦ä¹ """

    def __init__(self):
        self.label_encoder = LabelEncoder()
        self.vectorizer = None
        self.models = {}
        self.ensemble = None
        self.best_single_model = None
        self.best_single_name = None

    def prepare_data(self, df, test_size=0.2, use_high_confidence=True):
        """å‡†å¤‡æ•°æ®"""
        print("\n" + "=" * 60)
        print("ğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...")
        print("=" * 60)

        # å¯é€‰ï¼šåªä½¿ç”¨é«˜ç½®ä¿¡åº¦æ ·æœ¬
        if use_high_confidence:
            high_conf_df = df[df['label_confidence'] >= 0.5].copy()
            print(f"  é«˜ç½®ä¿¡åº¦æ ·æœ¬ (â‰¥0.5): {len(high_conf_df)} / {len(df)} ({len(high_conf_df)/len(df)*100:.1f}%)")
            working_df = high_conf_df
        else:
            working_df = df.copy()

        # åˆ›å»ºæ–‡æœ¬ç‰¹å¾
        working_df['combined_text'] = working_df.apply(create_features, axis=1)
        working_df['processed_text'] = working_df['combined_text'].apply(preprocess_text)

        # åˆ†å‰²æ•°æ®
        X_train_df, X_test_df = train_test_split(
            working_df, test_size=test_size, random_state=RANDOM_STATE,
            stratify=working_df['capacity_label']
        )

        # å¯¹è®­ç»ƒé›†è¿›è¡Œç±»åˆ«å¹³è¡¡
        X_train_df = balance_classes(X_train_df)

        # ç¼–ç æ ‡ç­¾
        y_train = self.label_encoder.fit_transform(X_train_df['capacity_label'])
        y_test = self.label_encoder.transform(X_test_df['capacity_label'])

        print(f"\n  è®­ç»ƒé›†: {len(X_train_df)} æ¡")
        print(f"  æµ‹è¯•é›†: {len(X_test_df)} æ¡")
        print(f"  ç±»åˆ«: {list(self.label_encoder.classes_)}")

        # TF-IDFç‰¹å¾ - ä¼˜åŒ–å‚æ•°
        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            ngram_range=(1, 3),
            min_df=2,
            max_df=0.9,
            sublinear_tf=True  # ä½¿ç”¨log(tf)
        )

        X_train = self.vectorizer.fit_transform(X_train_df['processed_text'])
        X_test = self.vectorizer.transform(X_test_df['processed_text'])

        print(f"  ç‰¹å¾ç»´åº¦: {X_train.shape[1]}")

        return X_train, X_test, y_train, y_test, X_test_df

    def train_models(self, X_train, y_train):
        """è®­ç»ƒå¤šä¸ªæ¨¡å‹"""
        print("\n" + "=" * 60)
        print("ğŸ¤– è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹...")
        print("=" * 60)

        # å®šä¹‰æ¨¡å‹
        model_configs = {
            'Logistic Regression': LogisticRegression(
                max_iter=2000, random_state=RANDOM_STATE,
                C=2.0, solver='lbfgs', class_weight='balanced'
            ),
            'Random Forest': RandomForestClassifier(
                n_estimators=300, max_depth=20, min_samples_split=5,
                random_state=RANDOM_STATE, n_jobs=-1, class_weight='balanced'
            ),
            'SVM': SVC(
                kernel='rbf', C=10, gamma='scale',
                random_state=RANDOM_STATE, probability=True, class_weight='balanced'
            ),
            'Gradient Boosting': GradientBoostingClassifier(
                n_estimators=200, max_depth=6, learning_rate=0.1,
                random_state=RANDOM_STATE
            ),
            'MLP': MLPClassifier(
                hidden_layer_sizes=(256, 128),
                max_iter=300, random_state=RANDOM_STATE,
                early_stopping=True, validation_fraction=0.1,
                activation='relu', solver='adam', alpha=0.001
            )
        }

        # è®­ç»ƒå„æ¨¡å‹
        results = []
        for name, model in model_configs.items():
            print(f"\n  è®­ç»ƒ {name}...")
            cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
            model.fit(X_train, y_train)
            self.models[name] = model

            result = {
                'model': name,
                'cv_accuracy': cv_scores.mean(),
                'cv_std': cv_scores.std()
            }
            results.append(result)
            print(f"    äº¤å‰éªŒè¯å‡†ç¡®ç‡: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")

        # æ‰¾å‡ºæœ€ä½³å•æ¨¡å‹
        best_result = max(results, key=lambda x: x['cv_accuracy'])
        self.best_single_name = best_result['model']
        self.best_single_model = self.models[self.best_single_name]
        print(f"\n  âœ… æœ€ä½³å•æ¨¡å‹: {self.best_single_name} (CVå‡†ç¡®ç‡: {best_result['cv_accuracy']:.4f})")

        # åˆ›å»ºé›†æˆæ¨¡å‹ï¼ˆæŠ•ç¥¨åˆ†ç±»å™¨ï¼‰
        print("\n  ğŸ”— åˆ›å»ºé›†æˆæ¨¡å‹ (Voting Classifier)...")
        self.ensemble = VotingClassifier(
            estimators=[
                ('lr', self.models['Logistic Regression']),
                ('rf', self.models['Random Forest']),
                ('svm', self.models['SVM']),
                ('gb', self.models['Gradient Boosting']),
            ],
            voting='soft',  # ä½¿ç”¨æ¦‚ç‡æŠ•ç¥¨
            weights=[1.2, 1.0, 1.2, 1.0]  # ç»™è¡¨ç°å¥½çš„æ¨¡å‹æ›´é«˜æƒé‡
        )
        self.ensemble.fit(X_train, y_train)

        ensemble_cv = cross_val_score(self.ensemble, X_train, y_train, cv=5, scoring='accuracy')
        print(f"    é›†æˆæ¨¡å‹äº¤å‰éªŒè¯å‡†ç¡®ç‡: {ensemble_cv.mean():.4f} (+/- {ensemble_cv.std():.4f})")

        return results

    def evaluate(self, X_test, y_test):
        """è¯„ä¼°æ¨¡å‹"""
        print("\n" + "=" * 60)
        print("ğŸ“ˆ æ¨¡å‹è¯„ä¼°ç»“æœ...")
        print("=" * 60)

        results = []

        # è¯„ä¼°å„æ¨¡å‹
        for name, model in self.models.items():
            y_pred = model.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)
            f1 = f1_score(y_test, y_pred, average='macro')

            results.append({
                'model': name,
                'accuracy': accuracy,
                'f1_macro': f1
            })
            print(f"  {name}: å‡†ç¡®ç‡={accuracy:.4f}, F1={f1:.4f}")

        # è¯„ä¼°é›†æˆæ¨¡å‹
        y_pred_ensemble = self.ensemble.predict(X_test)
        ensemble_accuracy = accuracy_score(y_test, y_pred_ensemble)
        ensemble_f1 = f1_score(y_test, y_pred_ensemble, average='macro')

        results.append({
            'model': 'Ensemble (Voting)',
            'accuracy': ensemble_accuracy,
            'f1_macro': ensemble_f1
        })
        print(f"\n  ğŸ† Ensemble (Voting): å‡†ç¡®ç‡={ensemble_accuracy:.4f}, F1={ensemble_f1:.4f}")

        # è¯¦ç»†æŠ¥å‘Š
        print(f"\n" + "=" * 60)
        print(f"ğŸ“‹ é›†æˆæ¨¡å‹è¯¦ç»†æŠ¥å‘Š:")
        print("=" * 60)

        print("\nåˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_test, y_pred_ensemble,
                                    target_names=self.label_encoder.classes_))

        print("\næ··æ·†çŸ©é˜µ:")
        cm = confusion_matrix(y_test, y_pred_ensemble)
        print(pd.DataFrame(cm,
                           index=self.label_encoder.classes_,
                           columns=self.label_encoder.classes_))

        return results, y_pred_ensemble

    def predict(self, texts, use_ensemble=True):
        """é¢„æµ‹"""
        processed = [preprocess_text(t) for t in texts]
        X = self.vectorizer.transform(processed)

        if use_ensemble:
            predictions = self.ensemble.predict(X)
        else:
            predictions = self.best_single_model.predict(X)

        labels = self.label_encoder.inverse_transform(predictions)
        return labels


# ============================================================================
# ç¬¬å››éƒ¨åˆ†ï¼šå¯è§†åŒ–
# ============================================================================

def create_visualizations(df_labeled, eval_results, y_test, y_pred, label_encoder):
    """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
    print("\n" + "=" * 60)
    print("ğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    print("=" * 60)

    fig, axes = plt.subplots(2, 2, figsize=(14, 12))

    # 1. æ ‡ç­¾åˆ†å¸ƒï¼ˆæŒ‰å¹´ä»½ï¼‰
    year_label_counts = df_labeled.groupby(['å¹´ä»½', 'capacity_label']).size().unstack(fill_value=0)
    year_label_counts.plot(kind='bar', ax=axes[0, 0], color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
    axes[0, 0].set_title('Label Distribution by Year', fontsize=12)
    axes[0, 0].set_xlabel('Year')
    axes[0, 0].set_ylabel('Count')
    axes[0, 0].legend(title='Capacity Type')
    axes[0, 0].tick_params(axis='x', rotation=0)

    # 2. æ¨¡å‹å¯¹æ¯”
    df_results = pd.DataFrame(eval_results)
    x = np.arange(len(df_results))
    width = 0.35
    axes[0, 1].bar(x - width/2, df_results['accuracy'], width, label='Accuracy', color='#4ECDC4')
    axes[0, 1].bar(x + width/2, df_results['f1_macro'], width, label='F1-Macro', color='#FF6B6B')
    axes[0, 1].set_xlabel('Model')
    axes[0, 1].set_ylabel('Score')
    axes[0, 1].set_title('Model Performance Comparison', fontsize=12)
    axes[0, 1].set_xticks(x)
    axes[0, 1].set_xticklabels(df_results['model'], rotation=45, ha='right')
    axes[0, 1].legend()
    axes[0, 1].set_ylim(0, 1.0)
    axes[0, 1].axhline(y=0.9, color='r', linestyle='--', alpha=0.7, label='90% Target')

    # 3. æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_test, y_pred)
    im = axes[1, 0].imshow(cm, interpolation='nearest', cmap='Blues')
    axes[1, 0].figure.colorbar(im, ax=axes[1, 0])
    labels = label_encoder.classes_
    axes[1, 0].set(xticks=np.arange(cm.shape[1]),
                   yticks=np.arange(cm.shape[0]),
                   xticklabels=labels, yticklabels=labels,
                   title='Confusion Matrix (Ensemble)',
                   ylabel='True Label',
                   xlabel='Predicted Label')
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            axes[1, 0].text(j, i, format(cm[i, j], 'd'),
                           ha="center", va="center",
                           color="white" if cm[i, j] > thresh else "black",
                           fontsize=12)

    # 4. æ¯ç±»å‡†ç¡®ç‡
    class_accuracy = []
    for i, label in enumerate(labels):
        mask = y_test == i
        if mask.sum() > 0:
            acc = (y_pred[mask] == i).sum() / mask.sum()
            class_accuracy.append(acc)
        else:
            class_accuracy.append(0)

    bars = axes[1, 1].bar(labels, class_accuracy, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
    axes[1, 1].set_title('Per-Class Accuracy', fontsize=12)
    axes[1, 1].set_xlabel('Capacity Type')
    axes[1, 1].set_ylabel('Accuracy')
    axes[1, 1].set_ylim(0, 1.0)
    axes[1, 1].axhline(y=0.9, color='r', linestyle='--', alpha=0.7)
    for bar, acc in zip(bars, class_accuracy):
        axes[1, 1].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,
                        f'{acc:.1%}', ha='center', va='bottom', fontsize=11)

    plt.tight_layout()
    plt.savefig('analysis_results_v3.png', dpi=150, bbox_inches='tight')
    plt.close()
    print("  å›¾è¡¨å·²ä¿å­˜: analysis_results_v3.png")


# ============================================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šä¸»ç¨‹åº
# ============================================================================

def main():
    """ä¸»ç¨‹åº"""
    print("\n" + "=" * 70)
    print("  æ”¿åºœé‡‡è´­åˆåŒå›½å®¶èƒ½åŠ›åˆ†ç±»å™¨ V3 - é«˜å‡†ç¡®åº¦ç‰ˆæœ¬")
    print("  ç›®æ ‡ï¼šå‡†ç¡®ç‡ > 90%")
    print("=" * 70)

    # 1. åŠ è½½æ•°æ®
    df = load_all_data()

    # 2. æ ‡æ³¨
    print("\n" + "=" * 60)
    print("ğŸ·ï¸  é«˜ç²¾åº¦è§„åˆ™æ ‡æ³¨...")
    print("=" * 60)

    labeler = HighAccuracyLabeler()
    df_labeled = labeler.label_dataframe(df)

    label_counts = df_labeled['capacity_label'].value_counts()
    print("\n  æ ‡ç­¾åˆ†å¸ƒ:")
    for label, count in label_counts.items():
        pct = count / len(df_labeled) * 100
        print(f"    {label}: {count} ({pct:.1f}%)")

    high_conf = df_labeled[df_labeled['label_confidence'] >= 0.5]
    print(f"\n  é«˜ç½®ä¿¡åº¦æ ·æœ¬ (â‰¥0.5): {len(high_conf)} ({len(high_conf)/len(df_labeled)*100:.1f}%)")

    # 3. è®­ç»ƒåˆ†ç±»å™¨
    classifier = HighAccuracyClassifier()
    X_train, X_test, y_train, y_test, test_df = classifier.prepare_data(
        df_labeled, test_size=0.15, use_high_confidence=True
    )

    cv_results = classifier.train_models(X_train, y_train)
    eval_results, y_pred = classifier.evaluate(X_test, y_test)

    # 4. æ£€æŸ¥æ˜¯å¦è¾¾åˆ°90%
    ensemble_result = [r for r in eval_results if 'Ensemble' in r['model']][0]
    accuracy = ensemble_result['accuracy']

    print("\n" + "=" * 70)
    if accuracy >= 0.9:
        print(f"  ğŸ‰ æˆåŠŸï¼å‡†ç¡®ç‡è¾¾åˆ° {accuracy:.2%}ï¼Œè¶…è¿‡90%ç›®æ ‡ï¼")
    else:
        print(f"  ğŸ“ˆ å½“å‰å‡†ç¡®ç‡: {accuracy:.2%}ï¼Œè·ç¦»90%ç›®æ ‡è¿˜å·® {0.9 - accuracy:.2%}")
    print("=" * 70)

    # 5. å¯è§†åŒ–
    create_visualizations(df_labeled, eval_results, y_test, y_pred, classifier.label_encoder)

    # 6. ç¤ºä¾‹é¢„æµ‹
    print("\n" + "=" * 60)
    print("ğŸ”® ç¤ºä¾‹é¢„æµ‹:")
    print("=" * 60)

    test_texts = [
        "ç¨åŠ¡å±€ä¿¡æ¯ç³»ç»Ÿå‡çº§æ”¹é€ é¡¹ç›®",
        "XXå°å­¦æ•™å­¦è®¾å¤‡é‡‡è´­é¡¹ç›® æ•™è‚²å±€",
        "å¸‚æ”¿é“è·¯ç»´ä¿®å·¥ç¨‹ ä½å»ºå±€",
        "XXåŒ»é™¢CTè®¾å¤‡é‡‡è´­ ç»¼åˆåŒ»é™¢",
        "è´¢æ”¿é¢„ç®—ç®¡ç†ç³»ç»Ÿé‡‡è´­ è´¢æ”¿å±€",
        "æ”¿åºœåŠå…¬å®¶å…·é‡‡è´­é¡¹ç›®",
        "ç–¾æ§ä¸­å¿ƒç–«è‹—å†·é“¾è®¾å¤‡é‡‡è´­",
        "å›½åœŸå±€åœŸåœ°ç¡®æƒç™»è®°ç³»ç»Ÿ",
    ]

    labels = classifier.predict(test_texts)

    label_names = {
        'extractive': 'æ±²å–èƒ½åŠ› (Extractive)',
        'coordination': 'åè°ƒèƒ½åŠ› (Coordination)',
        'compliance': 'åˆè§„èƒ½åŠ› (Compliance)'
    }

    for text, label in zip(test_texts, labels):
        print(f"  '{text[:25]}...' â†’ {label_names.get(label, label)}")

    # 7. ä¿å­˜ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ’¾ ä¿å­˜ç»“æœ...")
    print("=" * 60)

    output_cols = ['å¹´ä»½', 'åˆåŒåç§°', 'ä¸»è¦æ ‡çš„åç§°', 'é‡‡è´­äºº', 'ä¾›åº”å•†',
                   'æ‰€å±è¡Œä¸š', 'åˆåŒé‡‘é¢num_ä¸‡å…ƒ', 'capacity_label',
                   'label_confidence', 'label_reason']
    df_output = df_labeled[[c for c in output_cols if c in df_labeled.columns]]
    df_output.to_csv('classified_contracts_v3.csv', index=False, encoding='utf-8-sig')
    print("  åˆ†ç±»ç»“æœå·²ä¿å­˜: classified_contracts_v3.csv")

    pd.DataFrame(eval_results).to_csv('model_evaluation_v3.csv', index=False)
    print("  æ¨¡å‹è¯„ä¼°å·²ä¿å­˜: model_evaluation_v3.csv")

    # 8. æ‘˜è¦ç»Ÿè®¡
    print("\n" + "=" * 70)
    print("ğŸ“Š åˆ†ç±»ç»“æœæ‘˜è¦")
    print("=" * 70)

    summary = df_labeled.groupby(['å¹´ä»½', 'capacity_label']).size().unstack(fill_value=0)
    print(summary)

    print("\n" + "=" * 70)
    print("  âœ… åˆ†ç±»å®Œæˆ!")
    print("=" * 70)

    return df_labeled, classifier, accuracy


if __name__ == '__main__':
    df_labeled, classifier, accuracy = main()
