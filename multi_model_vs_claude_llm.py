#!/usr/bin/env python3
"""
å¤šæ¨¡å‹ä¸Claude LLMç†è§£åˆ†ç±»å¯¹æ¯”
éªŒè¯å„æ¨¡å‹ä¸Claude LLMåˆ†ç±»çš„ä¸€è‡´ç‡
"""

import pandas as pd
import numpy as np
import glob
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.naive_bayes import MultinomialNB
import warnings
warnings.filterwarnings('ignore')

# ========== 1. åŠ è½½è®­ç»ƒæ•°æ® ==========
print("=" * 70)
print("åŠ è½½è®­ç»ƒæ•°æ®...")
print("=" * 70)

all_files = glob.glob('classification_batch*.csv')
dfs = []
for file in sorted(all_files):
    try:
        df = pd.read_csv(file, on_bad_lines='skip')
        dfs.append(df)
    except Exception as e:
        print(f"  è­¦å‘Š: {e}")

train_df = pd.concat(dfs, ignore_index=True)
train_df = train_df.drop_duplicates(subset=['åºå·'])
train_df = train_df[train_df['åˆ†ç±»'] != 'å…¶ä»–']
train_df['text'] = train_df['é‡‡è´­äºº'].fillna('') + ' ' + train_df['åˆåŒåç§°'].fillna('')

print(f"è®­ç»ƒæ•°æ®: {len(train_df)} æ¡")

# ========== 2. åŠ è½½Claude LLMåˆ†ç±»ç»“æœ ==========
print("\n" + "=" * 70)
print("åŠ è½½Claude LLMç†è§£åˆ†ç±»ç»“æœ...")
print("=" * 70)

# ä»å·²ä¿å­˜çš„å¯¹æ¯”æ–‡ä»¶åŠ è½½Claudeåˆ†ç±»
claude_df = pd.read_csv('claude_llm_vs_ml_comparison.csv')
print(f"2012-2014å¹´æ•°æ®: {len(claude_df)} æ¡")
print(f"Claude LLMåˆ†ç±»åˆ†å¸ƒ:")
print(claude_df['Claude_LLMåˆ†ç±»'].value_counts())

# ========== 3. å‡†å¤‡æµ‹è¯•æ•°æ® ==========
# æ„å»ºæµ‹è¯•æ–‡æœ¬
claude_df['text'] = claude_df['é‡‡è´­äºº'].fillna('').astype(str) + ' ' + claude_df['åˆåŒåç§°'].fillna('').astype(str)

# ========== 4. å‘é‡åŒ– ==========
print("\n" + "=" * 70)
print("TF-IDFå‘é‡åŒ– (char 2-4 grams)...")
print("=" * 70)

vectorizer = TfidfVectorizer(
    analyzer='char',
    ngram_range=(2, 4),
    max_features=15000,
    min_df=2
)

X_train = vectorizer.fit_transform(train_df['text'])
X_test = vectorizer.transform(claude_df['text'])

le = LabelEncoder()
y_train = le.fit_transform(train_df['åˆ†ç±»'])
y_claude = claude_df['Claude_LLMåˆ†ç±»'].values

print(f"è®­ç»ƒç‰¹å¾: {X_train.shape}")
print(f"æµ‹è¯•ç‰¹å¾: {X_test.shape}")

# ========== 5. è®­ç»ƒå¤šä¸ªæ¨¡å‹å¹¶å¯¹æ¯” ==========
print("\n" + "=" * 70)
print("è®­ç»ƒå¤šä¸ªæ¨¡å‹å¹¶ä¸Claude LLMåˆ†ç±»å¯¹æ¯”...")
print("=" * 70)

models = {
    'LogisticRegression': LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000),
    'LinearSVC': LinearSVC(class_weight='balanced', random_state=42, max_iter=3000),
    'LinearSVC_C0.1': LinearSVC(class_weight='balanced', C=0.1, random_state=42, max_iter=3000),
    'LinearSVC_C10': LinearSVC(class_weight='balanced', C=10, random_state=42, max_iter=3000),
    'LogisticRegression_C10': LogisticRegression(class_weight='balanced', C=10, random_state=42, max_iter=1000),
    'SGD_hinge': SGDClassifier(loss='hinge', class_weight='balanced', random_state=42, max_iter=1000),
    'SGD_log': SGDClassifier(loss='log_loss', class_weight='balanced', random_state=42, max_iter=1000),
    'RandomForest': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42, n_jobs=-1),
    'RandomForest_200': RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42, n_jobs=-1),
    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
    'MultinomialNB': MultinomialNB(alpha=0.1),
}

results = []

for name, model in models.items():
    print(f"\nè®­ç»ƒ {name}...")
    model.fit(X_train, y_train)

    # é¢„æµ‹
    y_pred = model.predict(X_test)
    y_pred_labels = le.inverse_transform(y_pred)

    # è®¡ç®—ä¸€è‡´ç‡
    agree = (y_pred_labels == y_claude).sum()
    total = len(y_claude)
    rate = agree / total * 100

    results.append({
        'æ¨¡å‹': name,
        'ä¸€è‡´æ•°': agree,
        'æ€»æ•°': total,
        'ä¸Claude LLMä¸€è‡´ç‡': rate
    })

    print(f"  ä¸€è‡´: {agree}/{total} ({rate:.2f}%)")

    # ä¿å­˜æ¯ä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœ
    claude_df[f'{name}_é¢„æµ‹'] = y_pred_labels

# ========== 6. ç»“æœæ’åºå¹¶ä¿å­˜ ==========
results_df = pd.DataFrame(results)
results_df = results_df.sort_values('ä¸Claude LLMä¸€è‡´ç‡', ascending=False)

print("\n" + "=" * 70)
print("å¤šæ¨¡å‹ä¸Claude LLMç†è§£åˆ†ç±»ä¸€è‡´ç‡æ’å")
print("=" * 70)

for i, (_, row) in enumerate(results_df.iterrows(), 1):
    if i == 1:
        rank = "ğŸ¥‡"
    elif i == 2:
        rank = "ğŸ¥ˆ"
    elif i == 3:
        rank = "ğŸ¥‰"
    else:
        rank = f"{i}."
    print(f"{rank} {row['æ¨¡å‹']}: {row['ä¸€è‡´æ•°']}/{row['æ€»æ•°']} ({row['ä¸Claude LLMä¸€è‡´ç‡']:.2f}%)")

# ä¿å­˜ç»“æœ
results_df.to_csv('multi_model_vs_claude_llm_comparison.csv', index=False, encoding='utf-8-sig')
print(f"\nç»“æœå·²ä¿å­˜åˆ°: multi_model_vs_claude_llm_comparison.csv")

# ä¿å­˜è¯¦ç»†å¯¹æ¯”
claude_df.to_csv('all_models_predictions_vs_claude_llm.csv', index=False, encoding='utf-8-sig')
print(f"è¯¦ç»†é¢„æµ‹å·²ä¿å­˜åˆ°: all_models_predictions_vs_claude_llm.csv")

# ========== 7. åˆ†æä¸ä¸€è‡´è®°å½• ==========
print("\n" + "=" * 70)
print("æœ€ä½³æ¨¡å‹ vs Claude LLM ä¸ä¸€è‡´åˆ†æ")
print("=" * 70)

best_model = results_df.iloc[0]['æ¨¡å‹']
disagree = claude_df[claude_df[f'{best_model}_é¢„æµ‹'] != claude_df['Claude_LLMåˆ†ç±»']]

print(f"\n{best_model} ä¸ Claude LLM ä¸ä¸€è‡´çš„è®°å½• ({len(disagree)}æ¡):")
for idx, row in disagree.iterrows():
    buyer = str(row['é‡‡è´­äºº'])[:30]
    print(f"  {row.get('å¹´ä»½', '')}: {buyer}")
    print(f"    ML:{row[f'{best_model}_é¢„æµ‹']} vs Claude:{row['Claude_LLMåˆ†ç±»']}")
