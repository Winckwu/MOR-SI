#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¿åºœé‡‡è´­åˆåŒå›½å®¶èƒ½åŠ›åˆ†ç±»å™¨
State Capacity Contract Classifier

åŸºäº Berwick & Christia (2018) ã€ŠState Capacity Reduxã€‹è®ºæ–‡æ¡†æ¶
å°†æ”¿åºœé‡‡è´­åˆåŒæŒ‰ç…§ä¸‰ç§å›½å®¶èƒ½åŠ›è¿›è¡Œåˆ†ç±»ï¼š
- æ±²å–èƒ½åŠ› (Extractive Capacity): å›½å®¶è·å–èµ„æºçš„èƒ½åŠ›
- åè°ƒèƒ½åŠ› (Coordination Capacity): ç»„ç»‡é›†ä½“è¡ŒåŠ¨çš„èƒ½åŠ›
- åˆè§„èƒ½åŠ› (Compliance Capacity): ç¡®ä¿æœä»çš„èƒ½åŠ›

ä½œè€…: Claude AI
æ—¥æœŸ: 2025-11-28
"""

import pandas as pd
import numpy as np
import re
import warnings
warnings.filterwarnings('ignore')

# æœºå™¨å­¦ä¹ åº“
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.neural_network import MLPClassifier

# å¯è§†åŒ–
import matplotlib.pyplot as plt
import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False

# è®¾ç½®éšæœºç§å­
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

# ============================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šåˆ†ç±»è§„åˆ™å®šä¹‰
# ============================================================================

class StateCapacityLabeler:
    """
    åŸºäºè®ºæ–‡æ¡†æ¶çš„è§„åˆ™æ ‡æ³¨å™¨

    æ ¹æ® Berwick & Christia (2018) çš„ç†è®ºæ¡†æ¶ï¼š

    1. æ±²å–èƒ½åŠ› (Extractive):
       - æ ¸å¿ƒæ˜¯ç»Ÿæ²»è€…ä¸èµ„æºæŒæœ‰è€…ä¹‹é—´çš„å…³ç³»
       - æ¶‰åŠç¨æ”¶ã€è´¢æ”¿ã€èµ„æºè·å–ã€å®¡è®¡ç­‰
       - å…³é”®æŒ‡æ ‡ï¼šç¨æ”¶æ¯”ç‡ã€è´¢æ”¿æ”¶å…¥

    2. åè°ƒèƒ½åŠ› (Coordination):
       - ä¾èµ–å®˜åƒšä¸ç¤¾ä¼šæˆå‘˜çš„å…³ç³»
       - æ¶‰åŠåŸºç¡€è®¾æ–½ã€å…¬å…±æœåŠ¡åè°ƒã€å®˜åƒšä½“ç³»è¿ä½œ
       - éŸ¦ä¼¯å¼ä¸“ä¸šå®˜åƒšåˆ¶çš„åˆ¶åº¦è¡¨ç°

    3. åˆè§„èƒ½åŠ› (Compliance):
       - ç¡®ä¿å…¬æ°‘ã€ç²¾è‹±å’Œå®˜åƒšæœä»å›½å®¶ç›®æ ‡
       - ä¸»è¦æ¶‰åŠæ•™è‚²ã€åŒ»ç–—ç­‰å…¬å…±æœåŠ¡æä¾›
       - å®˜åƒšé€‰æ‹”ã€æ¿€åŠ±å’Œç›‘ç£æœºåˆ¶
    """

    def __init__(self):
        # æ±²å–èƒ½åŠ›å…³é”®è¯ï¼ˆä¸èµ„æºè·å–ã€ç¨æ”¶ã€è´¢æ”¿ç›¸å…³ï¼‰
        self.extractive_keywords = [
            # è´¢ç¨ç›¸å…³
            'ç¨åŠ¡', 'ç¨æ”¶', 'è´¢ç¨', 'è´¢æ”¿', 'é¢„ç®—', 'å®¡è®¡', 'ä¼šè®¡',
            'è´¢åŠ¡', 'èµ„é‡‘', 'æ”¶è´¹', 'ç¼´è´¹', 'ç½šæ¬¾', 'ç½šæ²¡',
            # èµ„äº§èµ„æºç›¸å…³
            'èµ„äº§', 'èµ„æº', 'çŸ¿äº§', 'å›½åœŸ', 'åœŸåœ°', 'æˆ¿äº§', 'ä¸åŠ¨äº§',
            'ç™»è®°', 'äº§æƒ', 'ç¡®æƒ', 'åœ°ç±', 'æµ‹ç»˜',
            # å¾æ”¶ç›¸å…³
            'å¾æ”¶', 'æ‹†è¿', 'è¡¥å¿', 'è¯„ä¼°',
            # é‡‘èç›¸å…³
            'é“¶è¡Œ', 'é‡‘è', 'è´·æ¬¾', 'èèµ„',
        ]

        # åè°ƒèƒ½åŠ›å…³é”®è¯ï¼ˆä¸åŸºç¡€è®¾æ–½ã€è¡Œæ”¿åè°ƒç›¸å…³ï¼‰
        self.coordination_keywords = [
            # åŸºç¡€è®¾æ–½
            'é“è·¯', 'å…¬è·¯', 'æ¡¥æ¢', 'éš§é“', 'äº¤é€š', 'è¿è¾“',
            'æ°´åˆ©', 'ç”µåŠ›', 'ä¾›ç”µ', 'ä¾›æ°´', 'æ’æ°´', 'ç®¡ç½‘', 'ç‡ƒæ°”',
            'é€šè®¯', 'é€šä¿¡', 'ç½‘ç»œ', 'ä¿¡æ¯åŒ–', 'ç”µå­æ”¿åŠ¡', 'æ•°å­—åŒ–',
            # å»ºè®¾å·¥ç¨‹
            'å»ºè®¾', 'å·¥ç¨‹', 'æ–½å·¥', 'æ”¹é€ ', 'ä¿®ç¼®', 'ç»´ä¿®',
            'è£…ä¿®', 'è£…é¥°', 'ç»¿åŒ–', 'ç¯å«', 'æ¸…æ´',
            # åŠå…¬åè°ƒ
            'åŠå…¬', 'è¡Œæ”¿', 'ä¼šè®®', 'æ¡£æ¡ˆ', 'å°åˆ·', 'æ‰“å°',
            'å¤å°', 'è®¾å¤‡', 'å®¶å…·', 'è½¦è¾†', 'åå‹¤',
            # è§„åˆ’ç®¡ç†
            'è§„åˆ’', 'è®¾è®¡', 'å’¨è¯¢', 'ç›‘ç†', 'ç®¡ç†',
        ]

        # åˆè§„èƒ½åŠ›å…³é”®è¯ï¼ˆä¸å…¬å…±æœåŠ¡æä¾›ã€ç›‘ç®¡æ‰§æ³•ç›¸å…³ï¼‰
        self.compliance_keywords = [
            # æ•™è‚²æœåŠ¡
            'æ•™è‚²', 'æ•™å­¦', 'å­¦æ ¡', 'åŸ¹è®­', 'è¯¾ç¨‹', 'æ•™æ', 'æ•™å…·',
            'å®éªŒ', 'å®è®­', 'å½•æ’­', 'å¤šåª’ä½“', 'å›¾ä¹¦', 'é˜…è¯»',
            'ä½“è‚²', 'è¿åŠ¨', 'å¥èº«',
            # åŒ»ç–—å«ç”Ÿ
            'åŒ»ç–—', 'åŒ»é™¢', 'å«ç”Ÿ', 'å¥åº·', 'ç–¾æ§', 'é˜²ç–«',
            'è¯Šæ–­', 'æ²»ç–—', 'æŠ¤ç†', 'åº·å¤', 'è¯å“', 'åŒ»è¯',
            'æ£€éªŒ', 'æ£€æµ‹', 'åŒ–éªŒ', 'å½±åƒ', 'CT', 'Bè¶…',
            # å…¬å…±å®‰å…¨
            'å®‰é˜²', 'ç›‘æ§', 'å®‰ä¿', 'æ¶ˆé˜²', 'åº”æ€¥', 'æ•‘æ´',
            'å…¬å®‰', 'è­¦åŠ¡', 'æ‰§æ³•', 'å¸æ³•', 'æ³•é™¢', 'æ£€å¯Ÿ',
            # ç¤¾ä¼šæœåŠ¡
            'å…»è€', 'ç¦åˆ©', 'æ•‘åŠ©', 'ç¤¾åŒº', 'æ°‘æ”¿',
            'ç¯ä¿', 'ç¯å¢ƒ', 'æ±¡æŸ“', 'åƒåœ¾', 'å¤„ç†',
        ]

        # è¡Œä¸šåˆ°èƒ½åŠ›ç±»å‹çš„æ˜ å°„
        self.industry_mapping = {
            # æ±²å–èƒ½åŠ›ç›¸å…³è¡Œä¸š
            'é‡‘èä¸š': 'extractive',
            'è´¢æ”¿': 'extractive',
            'ç¨åŠ¡': 'extractive',

            # åè°ƒèƒ½åŠ›ç›¸å…³è¡Œä¸š
            'å»ºç­‘ä¸š': 'coordination',
            'äº¤é€šè¿è¾“ä¸š': 'coordination',
            'é‚®æ”¿ä¸š': 'coordination',
            'ä¿¡æ¯ä¼ è¾“': 'coordination',
            'æˆ¿åœ°äº§ä¸š': 'coordination',
            'ç§Ÿèµå’Œå•†åŠ¡æœåŠ¡ä¸š': 'coordination',

            # åˆè§„èƒ½åŠ›ç›¸å…³è¡Œä¸š
            'æ™®é€šé«˜ç­‰æ•™è‚²': 'compliance',
            'ä¸­ç­‰èŒä¸šå­¦æ ¡æ•™è‚²': 'compliance',
            'æ™®é€šå°å­¦æ•™è‚²': 'compliance',
            'æ™®é€šåˆä¸­æ•™è‚²': 'compliance',
            'å­¦å‰æ•™è‚²': 'compliance',
            'ç‰¹æ®Šæ•™è‚²': 'compliance',
            'æ•™è‚²': 'compliance',
            'ç»¼åˆåŒ»é™¢': 'compliance',
            'ä¸“ç§‘åŒ»é™¢': 'compliance',
            'å«ç”Ÿ': 'compliance',
            'åŒ»ç–—': 'compliance',
            'ç¤¾ä¼šå·¥ä½œ': 'compliance',
            'å…¬å…±ç®¡ç†': 'compliance',
        }

    def _count_keywords(self, text, keywords):
        """ç»Ÿè®¡æ–‡æœ¬ä¸­å…³é”®è¯å‡ºç°æ¬¡æ•°"""
        if pd.isna(text):
            return 0
        text = str(text).lower()
        count = 0
        for keyword in keywords:
            count += len(re.findall(keyword.lower(), text))
        return count

    def label_single(self, contract_name, subject_name=None, industry=None):
        """
        å¯¹å•æ¡è®°å½•è¿›è¡Œæ ‡æ³¨

        è¿”å›: (æ ‡ç­¾, ç½®ä¿¡åº¦, åŸå› )
        """
        # åˆå¹¶æ–‡æœ¬
        text = str(contract_name) if contract_name else ''
        if subject_name and not pd.isna(subject_name):
            text += ' ' + str(subject_name)

        # ç»Ÿè®¡å„ç±»å…³é”®è¯
        extractive_score = self._count_keywords(text, self.extractive_keywords)
        coordination_score = self._count_keywords(text, self.coordination_keywords)
        compliance_score = self._count_keywords(text, self.compliance_keywords)

        # è¡Œä¸šåŠ æƒ
        industry_bonus = 0
        industry_label = None
        if industry and not pd.isna(industry):
            for ind_key, label in self.industry_mapping.items():
                if ind_key in str(industry):
                    industry_label = label
                    industry_bonus = 2
                    break

        # è®¡ç®—æœ€ç»ˆå¾—åˆ†
        scores = {
            'extractive': extractive_score + (industry_bonus if industry_label == 'extractive' else 0),
            'coordination': coordination_score + (industry_bonus if industry_label == 'coordination' else 0),
            'compliance': compliance_score + (industry_bonus if industry_label == 'compliance' else 0)
        }

        # ç¡®å®šæ ‡ç­¾
        max_score = max(scores.values())
        total_score = sum(scores.values())

        if max_score == 0:
            # æ— æ˜æ˜¾ç‰¹å¾ï¼Œæ ¹æ®è¡Œä¸šåˆ¤æ–­
            if industry_label:
                return industry_label, 0.3, 'ä»…è¡Œä¸šåŒ¹é…'
            else:
                return 'coordination', 0.1, 'é»˜è®¤åˆ†ç±»ï¼ˆé€šç”¨æ”¿åºœé‡‡è´­ï¼‰'

        # æ‰¾å‡ºæœ€é«˜åˆ†çš„ç±»åˆ«
        label = max(scores, key=scores.get)
        confidence = max_score / (total_score + 1)

        return label, confidence, f'å…³é”®è¯åŒ¹é…(E:{extractive_score},C:{coordination_score},P:{compliance_score})'

    def label_dataframe(self, df):
        """å¯¹æ•´ä¸ªæ•°æ®æ¡†è¿›è¡Œæ ‡æ³¨"""
        labels = []
        confidences = []
        reasons = []

        for idx, row in df.iterrows():
            label, conf, reason = self.label_single(
                row.get('åˆåŒåç§°', ''),
                row.get('ä¸»è¦æ ‡çš„åç§°', ''),
                row.get('æ‰€å±è¡Œä¸š', '')
            )
            labels.append(label)
            confidences.append(conf)
            reasons.append(reason)

        df_labeled = df.copy()
        df_labeled['capacity_label'] = labels
        df_labeled['label_confidence'] = confidences
        df_labeled['label_reason'] = reasons

        return df_labeled


# ============================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šæ•°æ®åŠ è½½å’Œé¢„å¤„ç†
# ============================================================================

def load_data():
    """åŠ è½½æ‰€æœ‰å¹´ä»½çš„æ•°æ®"""
    print("=" * 60)
    print("ğŸ“‚ åŠ è½½æ•°æ®...")
    print("=" * 60)

    dfs = []
    for year in ['2012', '2013', '2014']:
        df = pd.read_stata(f'{year}.dta')
        print(f"  {year}å¹´: {len(df)} æ¡è®°å½•")
        dfs.append(df)

    df_all = pd.concat(dfs, ignore_index=True)
    print(f"\n  æ€»è®¡: {len(df_all)} æ¡è®°å½•")
    return df_all


def preprocess_text(text):
    """æ–‡æœ¬é¢„å¤„ç†"""
    if pd.isna(text):
        return ''
    text = str(text)
    # å»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', ' ', text)
    # å»é™¤å¤šä½™ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def create_combined_text(row):
    """åˆ›å»ºç»„åˆæ–‡æœ¬ç‰¹å¾"""
    texts = []
    for col in ['åˆåŒåç§°', 'ä¸»è¦æ ‡çš„åç§°', 'æ‰€å±è¡Œä¸š', 'é‡‡è´­æ–¹å¼']:
        if col in row and not pd.isna(row[col]):
            texts.append(str(row[col]))
    return ' '.join(texts)


# ============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šç‰¹å¾å·¥ç¨‹
# ============================================================================

class FeatureExtractor:
    """ç‰¹å¾æå–å™¨"""

    def __init__(self, method='tfidf', max_features=5000):
        self.method = method
        self.max_features = max_features
        self.vectorizer = None

    def fit_transform(self, texts):
        """è®­ç»ƒå¹¶è½¬æ¢æ–‡æœ¬"""
        if self.method == 'tfidf':
            self.vectorizer = TfidfVectorizer(
                max_features=self.max_features,
                ngram_range=(1, 2),
                min_df=2,
                max_df=0.95
            )
        elif self.method == 'count':
            self.vectorizer = CountVectorizer(
                max_features=self.max_features,
                ngram_range=(1, 2),
                min_df=2,
                max_df=0.95
            )

        return self.vectorizer.fit_transform(texts)

    def transform(self, texts):
        """è½¬æ¢æ–‡æœ¬"""
        return self.vectorizer.transform(texts)

    def get_feature_names(self):
        """è·å–ç‰¹å¾åç§°"""
        return self.vectorizer.get_feature_names_out()


# ============================================================================
# ç¬¬å››éƒ¨åˆ†ï¼šæ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°
# ============================================================================

class StateCapacityClassifier:
    """å›½å®¶èƒ½åŠ›åˆ†ç±»å™¨"""

    def __init__(self):
        self.label_encoder = LabelEncoder()
        self.feature_extractor = None
        self.models = {}
        self.best_model = None
        self.best_model_name = None

    def prepare_data(self, df, test_size=0.2):
        """å‡†å¤‡è®­ç»ƒå’Œæµ‹è¯•æ•°æ®"""
        print("\n" + "=" * 60)
        print("ğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...")
        print("=" * 60)

        # åˆ›å»ºç»„åˆæ–‡æœ¬
        df['combined_text'] = df.apply(create_combined_text, axis=1)
        df['processed_text'] = df['combined_text'].apply(preprocess_text)

        # ç¼–ç æ ‡ç­¾
        y = self.label_encoder.fit_transform(df['capacity_label'])

        # åˆ†å‰²æ•°æ®
        X_train_text, X_test_text, y_train, y_test = train_test_split(
            df['processed_text'].values,
            y,
            test_size=test_size,
            random_state=RANDOM_STATE,
            stratify=y
        )

        print(f"  è®­ç»ƒé›†: {len(X_train_text)} æ¡")
        print(f"  æµ‹è¯•é›†: {len(X_test_text)} æ¡")
        print(f"  ç±»åˆ«åˆ†å¸ƒ:")
        for i, label in enumerate(self.label_encoder.classes_):
            train_count = sum(y_train == i)
            test_count = sum(y_test == i)
            print(f"    {label}: è®­ç»ƒ={train_count}, æµ‹è¯•={test_count}")

        # ç‰¹å¾æå–
        self.feature_extractor = FeatureExtractor(method='tfidf', max_features=3000)
        X_train = self.feature_extractor.fit_transform(X_train_text)
        X_test = self.feature_extractor.transform(X_test_text)

        print(f"\n  ç‰¹å¾ç»´åº¦: {X_train.shape[1]}")

        return X_train, X_test, y_train, y_test

    def train_models(self, X_train, y_train):
        """è®­ç»ƒå¤šä¸ªæ¨¡å‹"""
        print("\n" + "=" * 60)
        print("ğŸ¤– è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹...")
        print("=" * 60)

        # å®šä¹‰æ¨¡å‹
        model_configs = {
            'Logistic Regression': LogisticRegression(
                max_iter=1000,
                random_state=RANDOM_STATE,
                class_weight='balanced'
            ),
            'Random Forest': RandomForestClassifier(
                n_estimators=100,
                max_depth=20,
                random_state=RANDOM_STATE,
                class_weight='balanced'
            ),
            'SVM': SVC(
                kernel='linear',
                random_state=RANDOM_STATE,
                class_weight='balanced',
                probability=True
            ),
            'Naive Bayes': MultinomialNB(alpha=0.1),
            'Gradient Boosting': GradientBoostingClassifier(
                n_estimators=100,
                max_depth=5,
                random_state=RANDOM_STATE
            ),
            'MLP Neural Network': MLPClassifier(
                hidden_layer_sizes=(256, 128),
                max_iter=500,
                random_state=RANDOM_STATE,
                early_stopping=True
            )
        }

        # è®­ç»ƒæ¯ä¸ªæ¨¡å‹
        results = []
        for name, model in model_configs.items():
            print(f"\n  è®­ç»ƒ {name}...")

            # äº¤å‰éªŒè¯
            cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1_macro')

            # å®Œæ•´è®­ç»ƒ
            model.fit(X_train, y_train)
            self.models[name] = model

            result = {
                'model': name,
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std()
            }
            results.append(result)
            print(f"    äº¤å‰éªŒè¯ F1: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")

        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_result = max(results, key=lambda x: x['cv_mean'])
        self.best_model_name = best_result['model']
        self.best_model = self.models[self.best_model_name]

        print(f"\n  âœ… æœ€ä½³æ¨¡å‹: {self.best_model_name}")

        return results

    def evaluate(self, X_test, y_test):
        """è¯„ä¼°æ‰€æœ‰æ¨¡å‹"""
        print("\n" + "=" * 60)
        print("ğŸ“ˆ æ¨¡å‹è¯„ä¼°ç»“æœ...")
        print("=" * 60)

        results = []
        for name, model in self.models.items():
            y_pred = model.predict(X_test)

            accuracy = accuracy_score(y_test, y_pred)
            f1 = f1_score(y_test, y_pred, average='macro')

            results.append({
                'model': name,
                'accuracy': accuracy,
                'f1_macro': f1
            })

            print(f"\n  {name}:")
            print(f"    å‡†ç¡®ç‡: {accuracy:.4f}")
            print(f"    F1-Macro: {f1:.4f}")

        # æœ€ä½³æ¨¡å‹è¯¦ç»†æŠ¥å‘Š
        print(f"\n" + "=" * 60)
        print(f"ğŸ“‹ æœ€ä½³æ¨¡å‹ ({self.best_model_name}) è¯¦ç»†æŠ¥å‘Š:")
        print("=" * 60)

        y_pred_best = self.best_model.predict(X_test)
        print("\nåˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(
            y_test,
            y_pred_best,
            target_names=self.label_encoder.classes_
        ))

        print("\næ··æ·†çŸ©é˜µ:")
        cm = confusion_matrix(y_test, y_pred_best)
        print(pd.DataFrame(
            cm,
            index=self.label_encoder.classes_,
            columns=self.label_encoder.classes_
        ))

        return results

    def predict(self, texts):
        """é¢„æµ‹æ–°æ–‡æœ¬"""
        processed = [preprocess_text(t) for t in texts]
        X = self.feature_extractor.transform(processed)
        predictions = self.best_model.predict(X)
        labels = self.label_encoder.inverse_transform(predictions)

        # å¦‚æœæ¨¡å‹æ”¯æŒæ¦‚ç‡é¢„æµ‹
        if hasattr(self.best_model, 'predict_proba'):
            probs = self.best_model.predict_proba(X)
            return labels, probs

        return labels, None

    def get_important_features(self, top_n=20):
        """è·å–é‡è¦ç‰¹å¾"""
        feature_names = self.feature_extractor.get_feature_names()

        if hasattr(self.best_model, 'feature_importances_'):
            importances = self.best_model.feature_importances_
        elif hasattr(self.best_model, 'coef_'):
            # å¯¹äºå¤šåˆ†ç±»ï¼Œå–å„ç±»åˆ«ç³»æ•°çš„å¹³å‡ç»å¯¹å€¼
            importances = np.abs(self.best_model.coef_).mean(axis=0)
        else:
            return None

        # æ’åº
        indices = np.argsort(importances)[::-1][:top_n]

        return [(feature_names[i], importances[i]) for i in indices]


# ============================================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šå¯è§†åŒ–
# ============================================================================

def plot_label_distribution(df, save_path='label_distribution.png'):
    """ç»˜åˆ¶æ ‡ç­¾åˆ†å¸ƒå›¾"""
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # æ ‡ç­¾åˆ†å¸ƒ
    label_counts = df['capacity_label'].value_counts()
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']

    axes[0].bar(label_counts.index, label_counts.values, color=colors)
    axes[0].set_title('å›½å®¶èƒ½åŠ›ç±»å‹åˆ†å¸ƒ', fontsize=14)
    axes[0].set_xlabel('èƒ½åŠ›ç±»å‹')
    axes[0].set_ylabel('åˆåŒæ•°é‡')

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, v in enumerate(label_counts.values):
        axes[0].text(i, v + 5, str(v), ha='center', fontsize=12)

    # ç½®ä¿¡åº¦åˆ†å¸ƒ
    for label in df['capacity_label'].unique():
        subset = df[df['capacity_label'] == label]
        axes[1].hist(subset['label_confidence'], bins=20, alpha=0.5, label=label)

    axes[1].set_title('æ ‡æ³¨ç½®ä¿¡åº¦åˆ†å¸ƒ', fontsize=14)
    axes[1].set_xlabel('ç½®ä¿¡åº¦')
    axes[1].set_ylabel('æ•°é‡')
    axes[1].legend()

    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"  å›¾è¡¨å·²ä¿å­˜: {save_path}")


def plot_model_comparison(results, save_path='model_comparison.png'):
    """ç»˜åˆ¶æ¨¡å‹å¯¹æ¯”å›¾"""
    df_results = pd.DataFrame(results)

    fig, ax = plt.subplots(figsize=(12, 6))

    x = np.arange(len(df_results))
    width = 0.35

    bars1 = ax.bar(x - width/2, df_results['accuracy'], width, label='å‡†ç¡®ç‡', color='#4ECDC4')
    bars2 = ax.bar(x + width/2, df_results['f1_macro'], width, label='F1-Macro', color='#FF6B6B')

    ax.set_xlabel('æ¨¡å‹')
    ax.set_ylabel('åˆ†æ•°')
    ax.set_title('æ¨¡å‹æ€§èƒ½å¯¹æ¯”')
    ax.set_xticks(x)
    ax.set_xticklabels(df_results['model'], rotation=45, ha='right')
    ax.legend()
    ax.set_ylim(0, 1.0)

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar in bars1:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.3f}', ha='center', va='bottom', fontsize=9)

    for bar in bars2:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.3f}', ha='center', va='bottom', fontsize=9)

    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"  å›¾è¡¨å·²ä¿å­˜: {save_path}")


def plot_confusion_matrix(y_true, y_pred, labels, save_path='confusion_matrix.png'):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾"""
    cm = confusion_matrix(y_true, y_pred)

    fig, ax = plt.subplots(figsize=(8, 6))

    im = ax.imshow(cm, interpolation='nearest', cmap='Blues')
    ax.figure.colorbar(im, ax=ax)

    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           xticklabels=labels, yticklabels=labels,
           title='æ··æ·†çŸ©é˜µ',
           ylabel='çœŸå®æ ‡ç­¾',
           xlabel='é¢„æµ‹æ ‡ç­¾')

    # åœ¨æ¯ä¸ªæ ¼å­ä¸­æ˜¾ç¤ºæ•°å€¼
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], 'd'),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"  å›¾è¡¨å·²ä¿å­˜: {save_path}")


# ============================================================================
# ç¬¬å…­éƒ¨åˆ†ï¼šä¸»ç¨‹åº
# ============================================================================

def main():
    """ä¸»ç¨‹åº"""
    print("\n" + "=" * 70)
    print("  æ”¿åºœé‡‡è´­åˆåŒå›½å®¶èƒ½åŠ›åˆ†ç±»å™¨")
    print("  åŸºäº Berwick & Christia (2018) ã€ŠState Capacity Reduxã€‹")
    print("=" * 70)

    # 1. åŠ è½½æ•°æ®
    df = load_data()

    # 2. ä½¿ç”¨è§„åˆ™è¿›è¡Œåˆå§‹æ ‡æ³¨ï¼ˆç”Ÿæˆè®­ç»ƒæ•°æ®ï¼‰
    print("\n" + "=" * 60)
    print("ğŸ·ï¸  åŸºäºè§„åˆ™çš„åˆå§‹æ ‡æ³¨...")
    print("=" * 60)

    labeler = StateCapacityLabeler()
    df_labeled = labeler.label_dataframe(df)

    # æ ‡ç­¾ç»Ÿè®¡
    print("\n  æ ‡ç­¾åˆ†å¸ƒ:")
    label_counts = df_labeled['capacity_label'].value_counts()
    for label, count in label_counts.items():
        pct = count / len(df_labeled) * 100
        print(f"    {label}: {count} ({pct:.1f}%)")

    # é«˜ç½®ä¿¡åº¦æ ·æœ¬ç»Ÿè®¡
    high_conf = df_labeled[df_labeled['label_confidence'] >= 0.3]
    print(f"\n  é«˜ç½®ä¿¡åº¦æ ·æœ¬ (â‰¥0.3): {len(high_conf)} ({len(high_conf)/len(df_labeled)*100:.1f}%)")

    # 3. å¯è§†åŒ–æ ‡ç­¾åˆ†å¸ƒ
    print("\n" + "=" * 60)
    print("ğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    print("=" * 60)
    plot_label_distribution(df_labeled)

    # 4. è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹
    classifier = StateCapacityClassifier()
    X_train, X_test, y_train, y_test = classifier.prepare_data(df_labeled)

    # 5. è®­ç»ƒæ¨¡å‹
    cv_results = classifier.train_models(X_train, y_train)

    # 6. è¯„ä¼°æ¨¡å‹
    eval_results = classifier.evaluate(X_test, y_test)

    # 7. ç»˜åˆ¶æ¨¡å‹å¯¹æ¯”å›¾
    plot_model_comparison(eval_results)

    # 8. ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    y_pred = classifier.best_model.predict(X_test)
    plot_confusion_matrix(y_test, y_pred, classifier.label_encoder.classes_)

    # 9. æ˜¾ç¤ºé‡è¦ç‰¹å¾
    print("\n" + "=" * 60)
    print("ğŸ”‘ é‡è¦ç‰¹å¾ (Top 20):")
    print("=" * 60)
    important_features = classifier.get_important_features(top_n=20)
    if important_features:
        for i, (feature, importance) in enumerate(important_features, 1):
            print(f"  {i:2d}. {feature}: {importance:.4f}")

    # 10. ä¿å­˜ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ’¾ ä¿å­˜ç»“æœ...")
    print("=" * 60)

    # ä¿å­˜æ ‡æ³¨åçš„æ•°æ®
    output_cols = ['å¹´ä»½', 'åˆåŒåç§°', 'ä¸»è¦æ ‡çš„åç§°', 'é‡‡è´­äºº', 'ä¾›åº”å•†',
                   'æ‰€å±è¡Œä¸š', 'åˆåŒé‡‘é¢num_ä¸‡å…ƒ', 'capacity_label',
                   'label_confidence', 'label_reason']
    df_output = df_labeled[[c for c in output_cols if c in df_labeled.columns]]
    df_output.to_csv('classified_contracts.csv', index=False, encoding='utf-8-sig')
    print("  åˆ†ç±»ç»“æœå·²ä¿å­˜: classified_contracts.csv")

    # ä¿å­˜æ¨¡å‹è¯„ä¼°ç»“æœ
    pd.DataFrame(eval_results).to_csv('model_evaluation.csv', index=False)
    print("  æ¨¡å‹è¯„ä¼°å·²ä¿å­˜: model_evaluation.csv")

    # 11. ç¤ºä¾‹é¢„æµ‹
    print("\n" + "=" * 60)
    print("ğŸ”® ç¤ºä¾‹é¢„æµ‹:")
    print("=" * 60)

    test_texts = [
        "ç¨åŠ¡ç³»ç»Ÿå‡çº§æ”¹é€ é¡¹ç›®",
        "å°å­¦æ•™å­¦è®¾å¤‡é‡‡è´­",
        "å¸‚æ”¿é“è·¯ç»´ä¿®å·¥ç¨‹",
        "åŒ»é™¢åŒ»ç–—å™¨æ¢°é‡‡è´­",
        "è´¢æ”¿é¢„ç®—ç®¡ç†ç³»ç»Ÿ",
        "åŠå…¬å®¶å…·é‡‡è´­"
    ]

    labels, probs = classifier.predict(test_texts)

    label_names_cn = {
        'extractive': 'æ±²å–èƒ½åŠ›',
        'coordination': 'åè°ƒèƒ½åŠ›',
        'compliance': 'åˆè§„èƒ½åŠ›'
    }

    for text, label in zip(test_texts, labels):
        print(f"  '{text}' â†’ {label_names_cn.get(label, label)}")

    print("\n" + "=" * 70)
    print("  âœ… åˆ†ç±»å®Œæˆ!")
    print("=" * 70)

    return df_labeled, classifier


if __name__ == '__main__':
    df_labeled, classifier = main()
